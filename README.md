# Semantic HTML Search Engine

<div align="center">

![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-009688.svg)
![Next.js](https://img.shields.io/badge/Next.js-latest-black.svg)
![Pinecone](https://img.shields.io/badge/Pinecone-Vector_DB-6366F1.svg)

A production-ready semantic search engine for HTML content using **Pinecone vector database**, sentence embeddings, FastAPI, and Next.js.

**Built for Smarter.codes Technical Assessment**

[Features](#-key-features) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [API](#-api-usage)

</div>

---

## ğŸ¯ Overview

This application demonstrates modern NLP techniques and vector database integration for semantic search:

- **Vector Database**: Pinecone serverless for scalable, persistent vector storage
- **Semantic Search**: sentence-transformers (all-MiniLM-L6-v2) for meaning-based similarity
- **HTML Processing**: Intelligent fetching and cleaning of web content
- **Smart Chunking**: BERT tokenizer-based text splitting (500 tokens max)
- **RESTful API**: FastAPI with automatic OpenAPI documentation
- **Modern Frontend**: Next.js SPA with beautiful gradient UI
- **Type Safety**: Full Pydantic models and TypeScript
- **Production Ready**: Comprehensive error handling and logging

---

## âœ¨ Key Features

### Backend
- âœ… **Pinecone Integration**: Cloud-native vector database with namespace support
- âœ… **500 Token Chunks**: BERT tokenizer with 50-token overlap
- âœ… **Semantic Embeddings**: 384-dimensional vectors (all-MiniLM-L6-v2)
- âœ… **URL-based Namespacing**: Organized indexing per website
- âœ… **Health Endpoints**: `/` and `/health` with vector store stats
- âœ… **CORS Support**: Configured for frontend integration

### Frontend
- âœ… **Modern UI**: Gradient design with animated backgrounds
- âœ… **Responsive Layout**: Mobile-first Tailwind CSS
- âœ… **Real-time Feedback**: Loading states and error handling
- âœ… **Result Visualization**: Score circles and relevance percentages
- âœ… **TypeScript**: Full type safety across components

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **Node.js 16+**
- **Pinecone Account** ([Sign up free](https://www.pinecone.io/))

### Option 1: Local Development (Recommended)

#### Step 1: Get Pinecone API Key

1. Sign up at [pinecone.io](https://www.pinecone.io/)
2. Create a new project
3. Copy your **API Key** from the dashboard
4. Note your preferred **cloud** (aws/gcp/azure) and **region**

#### Step 2: Setup Backend

```bash
# Clone the repository
git clone https://github.com/aaron-seq/semantic-html-search-smarter-codes.git
cd semantic-html-search-smarter-codes/backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
cp ../.env.example .env
```

**Edit `.env` file:**
```env
PINECONE_API_KEY=your_api_key_here
PINECONE_INDEX_NAME=html-search
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1
BACKEND_PORT=8000
LOG_LEVEL=INFO
```

```bash
# Run the backend
python app.py
```

âœ… Backend running at **http://localhost:8000**  
ğŸ“š API Docs at **http://localhost:8000/docs**

#### Step 3: Setup Frontend

```bash
# Open new terminal
cd frontend

# Install dependencies
npm install

# Create environment file
echo "NEXT_PUBLIC_API_URL=http://localhost:8000" > .env.local

# Run development server
npm run dev
```

âœ… Frontend running at **http://localhost:3000**

### Option 2: Docker Deployment

```bash
# Create .env file with Pinecone credentials
cp .env.example .env
# Edit .env with your PINECONE_API_KEY

# Build and start
docker-compose up -d

# Check status
docker-compose ps
```

---

## ğŸ“– Documentation

### Architecture

```
.
â”œâ”€â”€ backend/                 # FastAPI Backend
â”‚   â”œâ”€â”€ app.py              # Main application & endpoints
â”‚   â”œâ”€â”€ models.py           # Pydantic request/response models
â”‚   â”œâ”€â”€ html_utils.py       # HTML fetching and cleaning
â”‚   â”œâ”€â”€ chunking.py         # Text chunking with BERT tokenizer
â”‚   â”œâ”€â”€ vector_store.py     # Pinecone vector database integration
â”‚   â”œâ”€â”€ test_backend.py     # Comprehensive test suite
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ frontend/               # Next.js Frontend
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ index.tsx       # Main search interface (SPA)
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ SearchForm.tsx  # Modern search form component
â”‚       â””â”€â”€ ResultsList.tsx # Beautiful results display
â”œâ”€â”€ docker-compose.yml      # Docker orchestration
â”œâ”€â”€ Dockerfile             # Backend container image
â””â”€â”€ README.md              # This file
```

### Technology Stack

**Backend:**
- Framework: FastAPI 0.104.1
- Vector Database: Pinecone (Serverless)
- Embeddings: sentence-transformers (all-MiniLM-L6-v2)
- HTML Parsing: BeautifulSoup4
- Tokenization: Transformers (BERT tokenizer)

**Frontend:**
- Framework: Next.js (React)
- Styling: Tailwind CSS
- Language: TypeScript

---

## ğŸ”§ API Usage

### Health Check

```bash
curl http://localhost:8000/
```

**Response:**
```json
{
  "status": "healthy",
  "service": "Semantic HTML Search",
  "version": "2.0.0",
  "vector_db": "Pinecone"
}
```

### Detailed Health

```bash
curl http://localhost:8000/health
```

**Response:**
```json
{
  "status": "healthy",
  "vector_store": "connected",
  "stats": {
    "index_name": "html-search",
    "total_vector_count": 1234,
    "dimension": 384,
    "distance_metric": "cosine"
  }
}
```

### Search Endpoint

**POST** `/search`

**Request Body:**
```json
{
  "url": "https://en.wikipedia.org/wiki/Machine_learning",
  "query": "neural networks and deep learning",
  "top_k": 10
}
```

**Response:**
```json
{
  "url": "https://en.wikipedia.org/wiki/Machine_learning",
  "query": "neural networks and deep learning",
  "results": [
    {
      "chunk": {
        "text": "Deep learning is part of a broader family of machine learning methods...",
        "start": 0,
        "end": 156
      },
      "score": 0.856
    }
  ],
  "total_chunks": 42
}
```

**Example with curl:**
```bash
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "query": "neural networks and deep learning",
    "top_k": 5
  }'
```

---

## ğŸ¨ UI Screenshots

### Modern Search Interface
- **Gradient Background**: Animated blob effects
- **Clean Forms**: Rounded inputs with icons
- **Status Badges**: Real-time Pinecone connection indicator

### Results Display
- **Relevance Scores**: Circular progress indicators
- **Beautiful Cards**: Hover effects and animations
- **Metadata**: Position tracking and character counts

---

## ğŸ”¬ How It Works

### 1. HTML Processing
- Fetches HTML from the provided URL
- Cleans content using BeautifulSoup (removes scripts, styles, navigation)
- Extracts readable text content

### 2. Text Chunking
- Uses BERT tokenizer (bert-base-uncased)
- Maximum 500 tokens per chunk
- 50 token overlap between chunks (prevents information loss)
- Maintains chunk position metadata

### 3. Vector Embedding
- Model: **all-MiniLM-L6-v2** (384 dimensions)
- Generates semantic embeddings for each chunk
- Fast: ~5000 sentences/second on CPU

### 4. Pinecone Vector Database
- **Index**: `html-search` (configurable)
- **Namespaces**: URL-based for organization
- **Distance Metric**: Cosine similarity
- **Features**:
  - Serverless, auto-scaling
  - Fast approximate nearest neighbor search
  - Metadata filtering support
  - Scalable to millions of vectors

### 5. Semantic Search
- Generates embedding for the query
- Searches Pinecone using cosine similarity
- Returns top-k results ranked by relevance score

---

## ğŸ§ª Running Tests

```bash
cd backend
pytest test_backend.py -v
```

**Test Coverage:**
- âœ… API endpoint functionality
- âœ… Text chunking algorithm
- âœ… Vector store operations
- âœ… Error handling
- âœ… Edge cases

---

## ğŸ“Š Performance

- **First Request**: ~2-3 seconds (model loading)
- **Subsequent Requests**: <1 second for typical pages
- **Memory Usage**: ~500MB for embedding model
- **Pinecone Storage**: Serverless, auto-scaling
- **Chunking Speed**: ~10,000 tokens/second
- **Embedding Speed**: ~5,000 sentences/second (CPU)

---

## ğŸš¢ Production Deployment

### Environment Variables

**Required:**
```env
PINECONE_API_KEY=your_api_key_here
```

**Optional:**
```env
PINECONE_INDEX_NAME=html-search
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1
BACKEND_PORT=8000
LOG_LEVEL=INFO
FRONTEND_URL=https://your-frontend.com
```

### Scaling Recommendations

1. **Infrastructure**
   - Deploy backend on cloud platforms (AWS, GCP, Azure)
   - Use CDN for frontend assets
   - Add load balancer for multiple backend instances

2. **Performance**
   - GPU acceleration for embedding generation
   - Redis caching for frequently accessed embeddings
   - Async processing for background indexing

3. **Security**
   - Add authentication (API keys or OAuth2)
   - Rate limiting (Slowapi)
   - Input validation and sanitization
   - HTTPS/TLS encryption

4. **Monitoring**
   - Prometheus metrics
   - Grafana dashboards
   - Error tracking (Sentry)
   - Request logging

---

## ğŸ› Troubleshooting

### Issue: Pinecone connection failed

**Solution**: Verify API key and check index exists:
```bash
# Check environment variables
echo $PINECONE_API_KEY

# Verify in Pinecone dashboard
# https://app.pinecone.io/
```

### Issue: Model download fails

**Solution**: Models download automatically on first run. Ensure internet connection and sufficient disk space (~500MB).

### Issue: Out of memory

**Solution**: The embedding model requires ~500MB RAM. Close other applications or use a machine with more memory.

### Issue: Import errors

**Solution**: Ensure all dependencies are installed:
```bash
pip install -r requirements.txt
```

### Issue: Frontend can't connect to backend

**Solution**: Check CORS settings and verify `NEXT_PUBLIC_API_URL` in `.env.local`:
```bash
echo "NEXT_PUBLIC_API_URL=http://localhost:8000" > frontend/.env.local
```

---

## âœ… Technical Assignment Compliance

- âœ… **Frontend**: Next.js SPA with URL + query inputs
- âœ… **Backend**: FastAPI with Python
- âœ… **HTML Parsing**: BeautifulSoup for DOM extraction
- âœ… **Tokenization**: BERT tokenizer with 500 token chunks
- âœ… **Vector Database**: Pinecone for semantic search
- âœ… **Top 10 Results**: Returns ranked results with scores
- âœ… **Clean Content**: Removes scripts, styles, navigation
- âœ… **Setup Instructions**: Complete README with prerequisites
- âœ… **Modern UI**: Beautiful gradient design with animations

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file

---

## ğŸ‘¤ Author

**Aaron Sequeira**  
GitHub: [@aaron-seq](https://github.com/aaron-seq)

---

## ğŸ™ Acknowledgments

- Built for **Smarter.codes** technical assessment
- Uses Hugging Face transformers and sentence-transformers
- Powered by **Pinecone** vector database
- Inspired by modern semantic search applications

---

## ğŸ“š Additional Resources

- [Pinecone Documentation](https://docs.pinecone.io/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Next.js Documentation](https://nextjs.org/docs)
- [Sentence Transformers](https://www.sbert.net/)
- [Technical Assignment](./SUBMISSION_CHECKLIST.md)

---

<div align="center">

**Made with â¤ï¸ for Smarter.codes**

</div>
